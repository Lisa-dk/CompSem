{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Sense Disambiguation\n",
    "## Group 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import sys \n",
    "from io import open \n",
    "import os\n",
    "from logging import debug, info, warning, error\n",
    "import nltk \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "import gensim\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PARSE_LAYER: \n",
    "    SYM = 2 \n",
    "    SEM = 3 \n",
    "    CAT = 4 \n",
    "    SNS = 5 \n",
    "    ROL = 6\n",
    "    POS = 7\n",
    "    LEM = 8\n",
    "    W2V = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the strategies implemented for integrating a pre-trained word embedding in a supervised WSD system\n",
    "class STRATEGY:\n",
    "    AVERAGE = 0\n",
    "    FRAC_DECAY = 1\n",
    "    EXP_DECAY = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the repository of the data\n",
    "def get_conll_blocks(in_file, split_lines=True, add_doc=False):\n",
    "    '''Read a CoNLL formatted input file and return the list of lists per sentence/document'''\n",
    "    docs = []\n",
    "    cur_doc = []\n",
    "    doc_ids = []\n",
    "    num_lines = -1\n",
    "    for line in open(in_file, 'r', encoding=\"utf-8\"):\n",
    "        if not line.strip() and cur_doc:\n",
    "            docs.append(cur_doc)\n",
    "            cur_doc = []\n",
    "            doc_ids.append(num_lines)\n",
    "        elif line.strip().startswith('# newdoc'):\n",
    "            # Keep track of start of new documents in doc_ids\n",
    "            # We form a list of all sentences in docs, but at some\n",
    "            # point we have to put multi-sent docs in a single file\n",
    "            num_lines += 1\n",
    "            if add_doc:\n",
    "                cur_doc.append(line.strip())\n",
    "        elif not line.strip().startswith('#') and line.strip():\n",
    "            if len(line.split()) != 7:\n",
    "                raise ValueError(\"Line should always consist of 7 layer-values, found {0}\\n{1}\".format(len(line.split()), line.strip()))\n",
    "            if split_lines:\n",
    "                cur_doc.append(line.split())\n",
    "            else:\n",
    "                cur_doc.append(line.strip())\n",
    "    # Add left over one if there's not an ending last line\n",
    "    if cur_doc:\n",
    "        docs.append(cur_doc)\n",
    "        doc_ids.append(num_lines)\n",
    "    # If num_lines is never increased, this means that the # newdoc information was not added\n",
    "    # In that case we just assume the default of 1 doc per block\n",
    "    if num_lines == -1:\n",
    "        info(\"Assuming 1 document per CoNLL block\")\n",
    "        doc_ids = range(0, len(docs))\n",
    "    info(\"Extracted {0} sents, for {1} docs\".format(len(docs), doc_ids[-1] + 1))\n",
    "    return docs, doc_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(directory, filename):\n",
    "     path = os.path.join(directory, filename)\n",
    "     docs, docs_ids = get_conll_blocks(path)\n",
    "     return docs\n",
    "\n",
    "language = 'en'\n",
    "standard = 'gold'\n",
    "\n",
    "directory = os.path.join(\"./data/4.0.0\", language, standard)\n",
    "\n",
    "train_data = read_data(os.path.join(directory), \"train.conll\")\n",
    "test_data = read_data(os.path.join(directory), \"test.conll\")\n",
    "\n",
    "train_data_labels = [word[PARSE_LAYER.SNS] for sentence in train_data for word in sentence]\n",
    "test_data_labels = [word[PARSE_LAYER.SNS] for sentence in test_data for word in sentence]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocessed(filename):\n",
    "    data = list(csv.reader(open(filename, encoding=\"utf-8\")))\n",
    "    for idx_sen, sentence in enumerate(data):\n",
    "        for idx, word in enumerate(sentence):\n",
    "            word = word.replace('[', '').replace(']', '').replace('\\'', '').split(', ')\n",
    "            data[idx_sen][idx] = word\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocesing\n",
    "Run if no preprocessed data can be loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load word2vec model trained on Google News\n",
    "# Can be downloaded from https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing [1]\n",
    "# [1] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. \n",
    "#     Distributed Representations of Words and Phrases and their Compositionality. In Proceedings of NIPS, 2013.\n",
    "wv = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:79\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "File \u001b[0;32m<timed exec>:74\u001b[0m, in \u001b[0;36madd_pos_lemma\u001b[0;34m(data)\u001b[0m\n",
      "File \u001b[0;32m<timed exec>:23\u001b[0m, in \u001b[0;36mget_closest_w2v_word\u001b[0;34m(sentence, word_idx, n_neighbours, strategy)\u001b[0m\n",
      "File \u001b[0;32m<timed exec>:59\u001b[0m, in \u001b[0;36mget_closest_exp_decay\u001b[0;34m(sentence, word_idx, n_neighbours)\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/gensim/models/keyedvectors.py:849\u001b[0m, in \u001b[0;36mKeyedVectors.most_similar\u001b[0;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    846\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indexer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(topn, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    847\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m indexer\u001b[38;5;241m.\u001b[39mmost_similar(mean, topn)\n\u001b[0;32m--> 849\u001b[0m dists \u001b[38;5;241m=\u001b[39m \u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectors\u001b[49m\u001b[43m[\u001b[49m\u001b[43mclip_start\u001b[49m\u001b[43m:\u001b[49m\u001b[43mclip_end\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorms[clip_start:clip_end]\n\u001b[1;32m    850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m topn:\n\u001b[1;32m    851\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dists\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Converts UPenn tags to tags relevant to WordNet\n",
    "def upenn_to_wn_tag(tagged_sentence):\n",
    "    wn_tags = []\n",
    "    for tag in tagged_sentence:\n",
    "        if tag[1].startswith('J'):\n",
    "            wn_tags.append('a')\n",
    "        elif tag[1].startswith('V'):\n",
    "            wn_tags.append('v')\n",
    "        elif tag[1].startswith('N'):\n",
    "            wn_tags.append('n')\n",
    "        elif tag[1].startswith('R'):\n",
    "            wn_tags.append('r')\n",
    "        else:\n",
    "            wn_tags.append('o')\n",
    "\n",
    "    return wn_tags\n",
    "\n",
    "\n",
    "# Uses word2vec word embeddings to find the closest word based on context\n",
    "def get_closest_w2v_word(sentence, word_idx, n_neighbours=2, strategy=STRATEGY.AVERAGE):\n",
    "    if strategy == STRATEGY.EXP_DECAY:\n",
    "        return get_closest_exp_decay(sentence, word_idx, n_neighbours)\n",
    "    return get_closest_average(sentence, word_idx, n_neighbours)\n",
    "   \n",
    "    \n",
    "# Return the closest word based on context - average strategy\n",
    "def get_closest_average(sentence, word_idx, n_neighbours=2):\n",
    "    n = 0\n",
    "    context_vec = np.zeros((300,))\n",
    "\n",
    "    for neighbour_idx in range(word_idx - n_neighbours, word_idx + n_neighbours + 1):\n",
    "        if neighbour_idx >=0 and neighbour_idx < len(sentence) and neighbour_idx != word_idx:\n",
    "            try:\n",
    "                context_vec += wv[sentence[neighbour_idx][PARSE_LAYER.LEM]]\n",
    "                n += 1\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    if n != 0:\n",
    "        return wv.most_similar(positive=[context_vec/n], topn=1)[0][0]\n",
    "    else:\n",
    "        return sentence[word_idx][PARSE_LAYER.LEM]\n",
    "    \n",
    "    \n",
    "# Return the closest word based on exponential decay\n",
    "def get_closest_exp_decay(sentence, word_idx, n_neighbours=2):\n",
    "    alpha = 1 - pow(0.1, 1 / (n_neighbors - 1))\n",
    "    context_vec = np.zeros((300,))\n",
    "\n",
    "    for neighbour_idx in range(word_idx - n_neighbours, word_idx + n_neighbours + 1):\n",
    "        if neighbour_idx >=0 and neighbour_idx < len(sentence) and neighbour_idx != word_idx:\n",
    "            try:\n",
    "                context_vec += wv[sentence[neighbour_idx][PARSE_LAYER.LEM]] * pow((1 - alpha), abs(word_idx - neighbour_idx) - 1)\n",
    "            except:\n",
    "                pass\n",
    "    return wv.most_similar(positive=[context_vec], topn=1)[0][0]\n",
    "\n",
    "    \n",
    "# Adds the WordNet POS tags, lemmas and closest word2vec word to the original data\n",
    "def add_pos_lemma(data):\n",
    "    for sentence in data:\n",
    "        tagged_sentence = pos_tag([item[0] for item in sentence])\n",
    "        wn_tags = upenn_to_wn_tag(tagged_sentence)\n",
    "        for idx, word in enumerate(sentence):\n",
    "            word.append(wn_tags[idx])\n",
    "            if wn_tags[idx] != 'o':\n",
    "                lemma = WordNetLemmatizer().lemmatize(word[0], pos=wn_tags[idx])\n",
    "            else:\n",
    "                lemma = WordNetLemmatizer().lemmatize(word[0])\n",
    "            word.append(lemma.lower())\n",
    "            word.append(get_closest_w2v_word(sentence, idx, strategy=STRATEGY.EXP_DECAY))\n",
    "        \n",
    "    return data\n",
    "\n",
    "\n",
    "train_data = add_pos_lemma(train_data)\n",
    "test_data = add_pos_lemma(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"preprocessed_data/train_data_exp_3.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(train_data)\n",
    "\n",
    "with open(\"preprocessed_data/test_data_exp_3.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline\n",
    "Use the most frequent sense for that word based on its POS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_pred(data):\n",
    "    pred = []\n",
    "    for sentence in data:\n",
    "        for word in sentence:\n",
    "            if word[PARSE_LAYER.POS] != 'o':\n",
    "                syns = wn.synsets(word[PARSE_LAYER.LEM].replace('~', '_'), lang='eng', pos=word[PARSE_LAYER.POS])\n",
    "                if len(syns) > 0:\n",
    "                    pred.append(syns[0].name())\n",
    "                else:\n",
    "                    pred.append('O')\n",
    "            else:\n",
    "                pred.append('O')\n",
    "    return pred\n",
    "\n",
    "# Same as the function above, but can be used directly on the feature vectors.\n",
    "def baseline_preprocessed(data, word, pos_tags):\n",
    "    pred = []\n",
    "    for feature in data:\n",
    "        if pos_tags[feature[0]] != 'o':\n",
    "            syns = wn.synsets(word.replace('~', '_'), lang='eng', pos=pos_tags[feature[0]])\n",
    "            if len(syns) > 0:\n",
    "                pred.append(syns[0].name())\n",
    "            else:\n",
    "                pred.append('O')\n",
    "        else:\n",
    "            pred.append('O')\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF with neighbouring POS tags and word embeddings\n",
    "Use the lemma, POS tag and thematic role of the word alongside with the ones of two neighbors on the left and right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_labels(train_data, test_data):\n",
    "    all_data = train_data + test_data\n",
    "    all_lemmas = np.array([word[PARSE_LAYER.SYM] for sentence in all_data for word in sentence])\n",
    "    lemma_labels = np.unique(all_lemmas)\n",
    "    all_pos_tags = np.array([word[PARSE_LAYER.POS] for sentence in all_data for word in sentence])\n",
    "    pos_labels = np.unique(all_pos_tags)\n",
    "    all_w2v_words = np.array([word[PARSE_LAYER.W2V] for sentence in all_data for word in sentence])\n",
    "    w2v_labels = np.unique(all_w2v_words)\n",
    "\n",
    "    return lemma_labels, pos_labels, w2v_labels\n",
    "\n",
    "def get_features(data, lemma_labels, pos_labels, w2v_labels, n_neighbours=3,):\n",
    "    features = []\n",
    "\n",
    "    for sentence in data:\n",
    "        for word_idx in range(len(sentence)):\n",
    "            feature = []\n",
    "            feature.append(np.where(lemma_labels == sentence[word_idx][PARSE_LAYER.SYM])[0][0])\n",
    "            feature.append(np.where(pos_labels == sentence[word_idx][PARSE_LAYER.POS])[0][0])\n",
    "\n",
    "            for neighbour_idx in range(word_idx - n_neighbours, word_idx + n_neighbours + 1):\n",
    "                if neighbour_idx != word_idx:\n",
    "                    try:\n",
    "                        feature.append(np.where(pos_labels == sentence[neighbour_idx][PARSE_LAYER.POS])[0][0])\n",
    "                    except:\n",
    "                        feature.append(-1)\n",
    "            feature.append(np.where(w2v_labels == sentence[word_idx][PARSE_LAYER.W2V])[0][0])\n",
    "\n",
    "            features.append(feature)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy baseline: 0.616185310071837\n",
      "Accuracy RF: 0.8658554464154816\n"
     ]
    }
   ],
   "source": [
    "# train_data = load_preprocessed(\"preprocessed_data/train_data_3.csv\")\n",
    "# test_data = load_preprocessed(\"preprocessed_data/test_data_3.csv\")\n",
    "\n",
    "# print(\"Accuracy baseline:\", accuracy_score(test_data_labels, baseline_pred(test_data)))\n",
    "\n",
    "# # Random forest using just the lemma and pos\n",
    "# lemma_labels, pos_labels, w2v_labels = return_labels(train_data, test_data)\n",
    "# train_features = get_features(train_data, lemma_labels, pos_labels, w2v_labels)\n",
    "# test_features = get_features(test_data, lemma_labels, pos_labels, w2v_labels)\n",
    "# train_features_pos = []\n",
    "\n",
    "# for feature in train_features:\n",
    "#     train_features_pos.append([feature[0], feature[1]])\n",
    "\n",
    "# test_features_pos = []\n",
    "# for feature in test_features:\n",
    "#     test_features_pos.append([feature[0], feature[1]])\n",
    "\n",
    "# rf_pos = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "# rf_pos.fit(train_features_pos, train_data_labels)\n",
    "# pred = rf_pos.predict(test_features_pos)\n",
    "# print(\"Accuracy RF:\", accuracy_score(test_data_labels, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model for each word type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_based_dicts(data, labels):\n",
    "    data_dict = {}\n",
    "    labels_dict = {}\n",
    "    \n",
    "    for idx, feat_vec in enumerate(data):\n",
    "        key = feat_vec[0]  # Lemma\n",
    "        if key not in data_dict: \n",
    "            data_dict[key] = []\n",
    "            labels_dict[key] = []\n",
    "        # The features will be everything but the lemma\n",
    "        data_dict[key].append(feat_vec[1:])\n",
    "        labels_dict[key].append(labels[idx])\n",
    "\n",
    "    return data_dict, labels_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF classifier with context based on 3 on each side\n",
      "Validation accuracy: 0.1794931475288165\n",
      "RF classifier with context based on 4 on each side\n",
      "Validation accuracy: 0.17974555899891964\n",
      "RF classifier with context based on 5 on each side\n",
      "Validation accuracy: 0.17978463644220846\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(shuffle=True, random_state=42)\n",
    "\n",
    "for n_neighbors in [3, 4, 5]:\n",
    "    train_data = load_preprocessed(\"preprocessed_data/train_data_\" + str(n_neighbors) + \".csv\")\n",
    "    test_data = load_preprocessed(\"preprocessed_data/test_data_\" + str(n_neighbors) + \".csv\")\n",
    "    lemma_labels, pos_labels, w2v_labels = return_labels(train_data, test_data)\n",
    "    train_features = get_features(train_data, lemma_labels, pos_labels, w2v_labels)\n",
    "    test_features = get_features(test_data, lemma_labels, pos_labels, w2v_labels)\n",
    "    train_data_dict, train_labels_dict = get_word_based_dicts(train_features, train_data_labels)\n",
    "    test_data_dict, test_labels_dict = get_word_based_dicts(test_features, test_data_labels)\n",
    "    \n",
    "    print('RF classifier with context based on', n_neighbors, 'on each side')\n",
    "    accuracies_all = []\n",
    "    total_keys = 0\n",
    "    for key in test_data_dict:\n",
    "        if key in train_data_dict and len(test_data_dict[key]) >= 5:\n",
    "            total_keys += 1\n",
    "            for fold, (train_index, test_index) in enumerate(kf.split(train_data_dict[key])):\n",
    "                train_data_dict[key] = np.array(train_data_dict[key])\n",
    "                train_labels_dict[key] = np.array(train_labels_dict[key])\n",
    "                accuracies = []\n",
    "                correct = 0\n",
    "                number_of_labels = 0\n",
    "\n",
    "                rf = RandomForestClassifier(random_state=42)\n",
    "                rf.fit(train_data_dict[key][train_index], train_labels_dict[key][train_index])\n",
    "                pred_dict = rf.predict(train_data_dict[key][test_index])\n",
    "\n",
    "                correct += sum(i==j for i, j in zip(pred_dict, train_labels_dict[key][test_index]))\n",
    "                number_of_labels += len(pred_dict)\n",
    "                accuracies.append(correct/number_of_labels)\n",
    "            accuracies_all.append(sum(accuracies)/5)\n",
    "\n",
    "    print('Validation accuracy:', sum(accuracies_all)/total_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.901040903093388\n"
     ]
    }
   ],
   "source": [
    "# The cell outputs the accuracy of using our training method on the testing keys (words)\n",
    "# present in the training keys(words). Whenever a testing key is not present in the \n",
    "# training data, it uses the baseline.\n",
    "\n",
    "train_data = load_preprocessed(\"preprocessed_data/train_data_exp_3.csv\")\n",
    "test_data = load_preprocessed(\"preprocessed_data/test_data_exp_3.csv\")\n",
    "lemma_labels, pos_labels, w2v_labels = return_labels(train_data, test_data)\n",
    "train_features = get_features(train_data, lemma_labels, pos_labels, w2v_labels)\n",
    "test_features = get_features(test_data, lemma_labels, pos_labels, w2v_labels)\n",
    "train_data_dict, train_labels_dict = get_word_based_dicts(train_features, train_data_labels)\n",
    "test_data_dict, test_labels_dict = get_word_based_dicts(test_features, test_data_labels)\n",
    "\n",
    "rf_dict = {}\n",
    "pred_dict = {}\n",
    "correct = 0\n",
    "number_of_labels = 0\n",
    "for key in test_data_dict:\n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "    try:\n",
    "        rf.fit(train_data_dict[key], train_labels_dict[key])\n",
    "        pred_dict[key] = rf.predict(test_data_dict[key])\n",
    "    except:\n",
    "        pred_dict[key] = baseline_preprocessed(test_data_dict[key], lemma_labels[key], pos_labels)\n",
    "    correct += sum(i==j for i, j in zip(pred_dict[key], test_labels_dict[key]))\n",
    "    number_of_labels += len(pred_dict[key])\n",
    "    \n",
    "print(\"Accuracy:\", correct / number_of_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9307191568505889\n",
      "Tested 1226 out of 1580 total words present in testing.\n"
     ]
    }
   ],
   "source": [
    "# The cell only outputs the accuracy of using our training method on the testing keys (words)\n",
    "# present in the training keys(words).\n",
    "\n",
    "train_data = load_preprocessed(\"preprocessed_data/train_data_exp_3.csv\")\n",
    "test_data = load_preprocessed(\"preprocessed_data/test_data_exp_3.csv\")\n",
    "lemma_labels, pos_labels, w2v_labels = return_labels(train_data, test_data)\n",
    "train_features = get_features(train_data, lemma_labels, pos_labels, w2v_labels)\n",
    "test_features = get_features(test_data, lemma_labels, pos_labels, w2v_labels)\n",
    "train_data_dict, train_labels_dict = get_word_based_dicts(train_features, train_data_labels)\n",
    "test_data_dict, test_labels_dict = get_word_based_dicts(test_features, test_data_labels)\n",
    "\n",
    "rf_dict = {}\n",
    "pred_dict = {}\n",
    "correct = 0\n",
    "number_of_labels = 0\n",
    "n_trained_keys = 0\n",
    "for key in test_data_dict:\n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "    try:\n",
    "        rf.fit(train_data_dict[key], train_labels_dict[key])\n",
    "        pred_dict[key] = rf.predict(test_data_dict[key])\n",
    "        correct += sum(i==j for i, j in zip(pred_dict[key], test_labels_dict[key]))\n",
    "        number_of_labels += len(pred_dict[key])\n",
    "        n_trained_keys += 1\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "print(\"Accuracy:\", correct / number_of_labels)\n",
    "print(\"Tested\", str(n_trained_keys), \"out of\", str(len(test_data_dict)), \"total words present in testing.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d8df1c93d52095079b28c314882b9f8223933acddcf9ddd27c0be9d60904f8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
