{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Sense Disambiguation\n",
    "## Group 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\alina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\alina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\alina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\alina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import sys \n",
    "from io import open \n",
    "import os\n",
    "from logging import debug, info, warning, error\n",
    "import nltk \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "import gensim\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PARSE_LAYER: \n",
    "    SYM = 2 \n",
    "    SEM = 3 \n",
    "    CAT = 4 \n",
    "    SNS = 5 \n",
    "    ROL = 6\n",
    "    POS = 7\n",
    "    W2V = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the strategies implemented for integrating a pre-trained word embedding in a supervised WSD system\n",
    "class STRATEGY:\n",
    "    AVERAGE = 0\n",
    "    EXP_DECAY = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the repository of the data\n",
    "def get_conll_blocks(in_file, split_lines=True, add_doc=False):\n",
    "    '''Read a CoNLL formatted input file and return the list of lists per sentence/document'''\n",
    "    docs = []\n",
    "    cur_doc = []\n",
    "    doc_ids = []\n",
    "    num_lines = -1\n",
    "    for line in open(in_file, 'r', encoding=\"utf-8\"):\n",
    "        if not line.strip() and cur_doc:\n",
    "            docs.append(cur_doc)\n",
    "            cur_doc = []\n",
    "            doc_ids.append(num_lines)\n",
    "        elif line.strip().startswith('# newdoc'):\n",
    "            # Keep track of start of new documents in doc_ids\n",
    "            # We form a list of all sentences in docs, but at some\n",
    "            # point we have to put multi-sent docs in a single file\n",
    "            num_lines += 1\n",
    "            if add_doc:\n",
    "                cur_doc.append(line.strip())\n",
    "        elif not line.strip().startswith('#') and line.strip():\n",
    "            if len(line.split()) != 7:\n",
    "                raise ValueError(\"Line should always consist of 7 layer-values, found {0}\\n{1}\".format(len(line.split()), line.strip()))\n",
    "            if split_lines:\n",
    "                cur_doc.append(line.split())\n",
    "            else:\n",
    "                cur_doc.append(line.strip())\n",
    "    # Add left over one if there's not an ending last line\n",
    "    if cur_doc:\n",
    "        docs.append(cur_doc)\n",
    "        doc_ids.append(num_lines)\n",
    "    # If num_lines is never increased, this means that the # newdoc information was not added\n",
    "    # In that case we just assume the default of 1 doc per block\n",
    "    if num_lines == -1:\n",
    "        info(\"Assuming 1 document per CoNLL block\")\n",
    "        doc_ids = range(0, len(docs))\n",
    "    info(\"Extracted {0} sents, for {1} docs\".format(len(docs), doc_ids[-1] + 1))\n",
    "    return docs, doc_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(directory, filename):\n",
    "     path = os.path.join(directory, filename)\n",
    "     docs, docs_ids = get_conll_blocks(path)\n",
    "     return docs\n",
    "\n",
    "language = 'en'\n",
    "standard = 'gold'\n",
    "\n",
    "directory = os.path.join(\"./data/4.0.0\", language, standard)\n",
    "\n",
    "train_data = read_data(os.path.join(directory), \"train.conll\")\n",
    "test_data = read_data(os.path.join(directory), \"test.conll\")\n",
    "\n",
    "train_data_labels = [word[PARSE_LAYER.SNS] for sentence in train_data for word in sentence]\n",
    "test_data_labels = [word[PARSE_LAYER.SNS] for sentence in test_data for word in sentence]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocessed(filename):\n",
    "    data = list(csv.reader(open(filename, encoding=\"utf-8\")))\n",
    "    for idx_sen, sentence in enumerate(data):\n",
    "        for idx, word in enumerate(sentence):\n",
    "            word = word.replace('[', '').replace(']', '').replace('\\'', '').split(', ')\n",
    "            data[idx_sen][idx] = word\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocesing\n",
    "Run if no preprocessed data can be loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load word2vec model trained on Google News\n",
    "# Can be downloaded from https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing [1]\n",
    "# [1] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. \n",
    "#     Distributed Representations of Words and Phrases and their Compositionality. In Proceedings of NIPS, 2013.\n",
    "wv = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts UPenn tags to tags relevant to WordNet\n",
    "def upenn_to_wn_tag(tagged_sentence):\n",
    "    wn_tags = []\n",
    "    for tag in tagged_sentence:\n",
    "        if tag[1].startswith('J'):\n",
    "            wn_tags.append('a')\n",
    "        elif tag[1].startswith('V'):\n",
    "            wn_tags.append('v')\n",
    "        elif tag[1].startswith('N'):\n",
    "            wn_tags.append('n')\n",
    "        elif tag[1].startswith('R'):\n",
    "            wn_tags.append('r')\n",
    "        else:\n",
    "            wn_tags.append('o')\n",
    "\n",
    "    return wn_tags\n",
    "\n",
    "\n",
    "# Uses word2vec word embeddings to find the closest word based on context\n",
    "def get_closest_w2v_word(sentence, word_idx, n_neighbours=1, strategy=STRATEGY.AVERAGE):\n",
    "    if strategy == STRATEGY.EXP_DECAY:\n",
    "        return get_closest_exp_decay(sentence, word_idx, n_neighbours)\n",
    "    return get_closest_average(sentence, word_idx, n_neighbours)\n",
    "   \n",
    "    \n",
    "# Return the closest word based on context - average strategy\n",
    "def get_closest_average(sentence, word_idx, n_neighbours=5):\n",
    "    n = 0 # counts how many neighbours were found in w2v and included in the context\n",
    "    context_vec = np.zeros((300,))\n",
    "\n",
    "    for neighbour_idx in range(word_idx - n_neighbours, word_idx + n_neighbours + 1):\n",
    "        if neighbour_idx >=0 and neighbour_idx < len(sentence) and neighbour_idx != word_idx:\n",
    "            try:\n",
    "                context_vec += wv[sentence[neighbour_idx][PARSE_LAYER.SYM]]\n",
    "                n += 1\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    if n != 0:\n",
    "        return wv.most_similar(positive=[context_vec/n], topn=1)[0][0]\n",
    "    else:\n",
    "        return sentence[word_idx][PARSE_LAYER.SYM]\n",
    "    \n",
    "    \n",
    "# Return the closest word based on exponential decay\n",
    "def get_closest_exp_decay(sentence, word_idx, n_neighbours=3):\n",
    "    alpha = 1 - pow(0.1, 1 / (n_neighbours - 1))\n",
    "    n = 0 # counts how many neighbours were found in w2v and included in the context\n",
    "    context_vec = np.zeros((300,))\n",
    "\n",
    "    for neighbour_idx in range(word_idx - n_neighbours, word_idx + n_neighbours + 1):\n",
    "        if neighbour_idx >=0 and neighbour_idx < len(sentence) and neighbour_idx != word_idx:\n",
    "            try:\n",
    "                context_vec += wv[sentence[neighbour_idx][PARSE_LAYER.SYM]] * pow((1 - alpha), abs(word_idx - neighbour_idx) - 1)\n",
    "                n += 1\n",
    "            except:\n",
    "                pass\n",
    "    if n != 0:\n",
    "        return wv.most_similar(positive=[context_vec], topn=1)[0][0]\n",
    "    else:\n",
    "        return sentence[word_idx][PARSE_LAYER.SYM]\n",
    "\n",
    "    \n",
    "# Adds the WordNet POS tags and closest word2vec word to the original data\n",
    "def add_pos_w2v(data):\n",
    "    for sentence in data:\n",
    "        tagged_sentence = pos_tag([item[0] for item in sentence])\n",
    "        wn_tags = upenn_to_wn_tag(tagged_sentence)\n",
    "        for idx, word in enumerate(sentence):\n",
    "            word.append(wn_tags[idx])\n",
    "            word.append(get_closest_w2v_word(sentence, idx, 5, strategy=STRATEGY.EXP_DECAY))\n",
    "    return data\n",
    "\n",
    "train_data = add_pos_w2v(test_data)\n",
    "test_data = add_pos_w2v(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"preprocessed_data/train_data_exp_5.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(train_data)\n",
    "\n",
    "with open(\"preprocessed_data/test_data_exp_5.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. of nouns in training = 12381\n",
      "Num. of verbs in training = 10020\n",
      "Num. of adjectives in training = 2287\n",
      "Num. of adverbs in training = 1540\n",
      "Num. of unlabeled in training = 24180\n",
      "\n",
      "Num. of nouns in testing = 1636\n",
      "Num. of verbs in testing = 1397\n",
      "Num. of adjectives in testing = 315\n",
      "Num. of adverbs in testing = 223\n",
      "Num. of unlabeled in testing = 3250\n"
     ]
    }
   ],
   "source": [
    "# Load any saved datasets.\n",
    "train_data = load_preprocessed(\"preprocessed_data/train_data_1.csv\")\n",
    "test_data = load_preprocessed(\"preprocessed_data/test_data_1.csv\")\n",
    "\n",
    "print(f\"Num. of nouns in training = {sum(w[PARSE_LAYER.POS] == 'n' for s in train_data for w in s)}\")\n",
    "print(f\"Num. of verbs in training = {sum(w[PARSE_LAYER.POS] == 'v' for s in train_data for w in s)}\")\n",
    "print(f\"Num. of adjectives in training = {sum(w[PARSE_LAYER.POS] == 'a' for s in train_data for w in s)}\")\n",
    "print(f\"Num. of adverbs in training = {sum(w[PARSE_LAYER.POS] == 'r' for s in train_data for w in s)}\")\n",
    "print(f\"Num. of unlabeled in training = {sum(w[PARSE_LAYER.POS] == 'o' for s in train_data for w in s)}\\n\")\n",
    "\n",
    "print(f\"Num. of nouns in testing = {sum(w[PARSE_LAYER.POS] == 'n' for s in test_data for w in s)}\")\n",
    "print(f\"Num. of verbs in testing = {sum(w[PARSE_LAYER.POS] == 'v' for s in test_data for w in s)}\")\n",
    "print(f\"Num. of adjectives in testing = {sum(w[PARSE_LAYER.POS] == 'a' for s in test_data for w in s)}\")\n",
    "print(f\"Num. of adverbs in testing = {sum(w[PARSE_LAYER.POS] == 'r' for s in test_data for w in s)}\")\n",
    "print(f\"Num. of unlabeled in testing = {sum(w[PARSE_LAYER.POS] == 'o' for s in test_data for w in s)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline\n",
    "Use the most frequent sense for that word based on its POS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_pred(data):\n",
    "    pred = []\n",
    "    for sentence in data:\n",
    "        for word in sentence:\n",
    "            if word[PARSE_LAYER.POS] != 'o':\n",
    "                syns = wn.synsets(word[PARSE_LAYER.SYM].replace('~', '_'), lang='eng', pos=word[PARSE_LAYER.POS])\n",
    "                if len(syns) > 0:\n",
    "                    pred.append(syns[0].name())\n",
    "                else:\n",
    "                    pred.append('O')\n",
    "            else:\n",
    "                pred.append('O')\n",
    "    return pred\n",
    "\n",
    "# Same as the function above, but can be used directly on the feature vectors.\n",
    "def baseline_preprocessed(data, word, pos_tags):\n",
    "    pred = []\n",
    "    for feature in data:\n",
    "        if pos_tags[feature[0]] != 'o':\n",
    "            syns = wn.synsets(word.replace('~', '_'), lang='eng', pos=pos_tags[feature[0]])\n",
    "            if len(syns) > 0:\n",
    "                pred.append(syns[0].name())\n",
    "            else:\n",
    "                pred.append('O')\n",
    "        else:\n",
    "            pred.append('O')\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF with neighbouring POS tags and word embeddings\n",
    "Use the lemma, POS tag and thematic role of the word alongside with the ones of two neighbors on the left and right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_labels(train_data, test_data):\n",
    "    all_data = train_data + test_data\n",
    "    all_lemmas = np.array([word[PARSE_LAYER.SYM] for sentence in all_data for word in sentence])\n",
    "    lemma_labels = np.unique(all_lemmas)\n",
    "    all_pos_tags = np.array([word[PARSE_LAYER.POS] for sentence in all_data for word in sentence])\n",
    "    pos_labels = np.unique(all_pos_tags)\n",
    "    all_w2v_words = np.array([word[PARSE_LAYER.W2V] for sentence in all_data for word in sentence])\n",
    "    w2v_labels = np.unique(all_w2v_words)\n",
    "\n",
    "    return lemma_labels, pos_labels, w2v_labels\n",
    "\n",
    "def get_features(data, lemma_labels, pos_labels, w2v_labels, n_neighbours=3):\n",
    "    features = []\n",
    "\n",
    "    for sentence in data:\n",
    "        for word_idx in range(len(sentence)):\n",
    "            feature = []\n",
    "            feature.append(np.where(lemma_labels == sentence[word_idx][PARSE_LAYER.SYM])[0][0])\n",
    "            feature.append(np.where(pos_labels == sentence[word_idx][PARSE_LAYER.POS])[0][0])\n",
    "\n",
    "            for neighbour_idx in range(word_idx - n_neighbours, word_idx + n_neighbours + 1):\n",
    "                if neighbour_idx != word_idx:\n",
    "                    try:\n",
    "                        feature.append(np.where(pos_labels == sentence[neighbour_idx][PARSE_LAYER.POS])[0][0])\n",
    "                    except:\n",
    "                        feature.append(-1)\n",
    "            feature.append(np.where(w2v_labels == sentence[word_idx][PARSE_LAYER.W2V])[0][0])\n",
    "\n",
    "            features.append(feature)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model for each word type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_based_dicts(data, labels):\n",
    "    data_dict = {}\n",
    "    labels_dict = {}\n",
    "    \n",
    "    for idx, feat_vec in enumerate(data):\n",
    "        key = feat_vec[0]  # Lemma\n",
    "        if key not in data_dict: \n",
    "            data_dict[key] = []\n",
    "            labels_dict[key] = []\n",
    "        # The features will be everything but the lemma\n",
    "        data_dict[key].append(feat_vec[1:])\n",
    "        labels_dict[key].append(labels[idx])\n",
    "\n",
    "    return data_dict, labels_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy baseline: 0.6195572496701364\n",
      "\n",
      "Accuracy nouns: 0.508557457212714\n",
      "Accuracy verbs: 0.2884753042233357\n",
      "Accuracy adj.: 0.4603174603174603\n",
      "Accuracy adv.: 0.16591928251121077\n",
      "Accuracy none: 0.8643076923076923\n"
     ]
    }
   ],
   "source": [
    "# Load any of the trained data sets for baseline\n",
    "train_data = load_preprocessed(\"preprocessed_data/train_data_1.csv\")\n",
    "test_data = load_preprocessed(\"preprocessed_data/test_data_1.csv\")\n",
    "\n",
    "print(f\"Accuracy baseline: {accuracy_score(test_data_labels, baseline_pred(test_data))}\\n\")\n",
    "\n",
    "test_n = [[test_data[i][j] for i in range(len(test_data)) for j in range(len(test_data[i]))\\\n",
    "    if test_data[i][j][PARSE_LAYER.POS] == 'n']]\n",
    "test_v = [[test_data[i][j] for i in range(len(test_data)) for j in range(len(test_data[i]))\\\n",
    "    if test_data[i][j][PARSE_LAYER.POS] == 'v']]\n",
    "test_a = [[test_data[i][j] for i in range(len(test_data)) for j in range(len(test_data[i]))\\\n",
    "    if test_data[i][j][PARSE_LAYER.POS] == 'a']]\n",
    "test_r = [[test_data[i][j] for i in range(len(test_data)) for j in range(len(test_data[i]))\\\n",
    "    if test_data[i][j][PARSE_LAYER.POS] == 'r']]\n",
    "test_o = [[test_data[i][j] for i in range(len(test_data)) for j in range(len(test_data[i]))\\\n",
    "    if test_data[i][j][PARSE_LAYER.POS] == 'o']]\n",
    "\n",
    "print(f\"Accuracy nouns: {accuracy_score([w[PARSE_LAYER.SNS] for w in test_n[0]], baseline_pred(test_n))}\")\n",
    "print(f\"Accuracy verbs: {accuracy_score([w[PARSE_LAYER.SNS] for w in test_v[0]], baseline_pred(test_v))}\")\n",
    "print(f\"Accuracy adj.: {accuracy_score([w[PARSE_LAYER.SNS] for w in test_a[0]], baseline_pred(test_a))}\")\n",
    "print(f\"Accuracy adv.: {accuracy_score([w[PARSE_LAYER.SNS] for w in test_r[0]], baseline_pred(test_r))}\")\n",
    "print(f\"Accuracy none: {accuracy_score([w[PARSE_LAYER.SNS] for w in test_o[0]], baseline_pred(test_o))}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### POS + context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (nn_w2v = 1, decay = 0, nn_pos = 0): 0.8840345990323999\n",
      "Accuracy (nn_w2v = 1, decay = 0, nn_pos = 1): 0.8957630845917021\n",
      "Accuracy (nn_w2v = 1, decay = 0, nn_pos = 3): 0.9019205395103357\n",
      "Accuracy (nn_w2v = 1, decay = 0, nn_pos = 5): 0.9035332062747398\n",
      "Accuracy (nn_w2v = 1, decay = 1, nn_pos = 0): 0.8840345990323999\n",
      "Accuracy (nn_w2v = 1, decay = 1, nn_pos = 1): 0.8957630845917021\n",
      "Accuracy (nn_w2v = 1, decay = 1, nn_pos = 3): 0.9019205395103357\n",
      "Accuracy (nn_w2v = 1, decay = 1, nn_pos = 5): 0.9035332062747398\n",
      "Accuracy (nn_w2v = 3, decay = 0, nn_pos = 0): 0.8806626594341006\n",
      "Accuracy (nn_w2v = 3, decay = 0, nn_pos = 1): 0.8915115085764551\n",
      "Accuracy (nn_w2v = 3, decay = 0, nn_pos = 3): 0.8986952059815276\n",
      "Accuracy (nn_w2v = 3, decay = 0, nn_pos = 5): 0.902067145579827\n",
      "Accuracy (nn_w2v = 3, decay = 1, nn_pos = 0): 0.883008356545961\n",
      "Accuracy (nn_w2v = 3, decay = 1, nn_pos = 1): 0.8945902360357719\n",
      "Accuracy (nn_w2v = 3, decay = 1, nn_pos = 3): 0.9008942970238968\n",
      "Accuracy (nn_w2v = 3, decay = 1, nn_pos = 5): 0.9038264184137224\n",
      "Accuracy (nn_w2v = 5, decay = 0, nn_pos = 0): 0.8793432048086791\n",
      "Accuracy (nn_w2v = 5, decay = 0, nn_pos = 1): 0.8918047207154376\n",
      "Accuracy (nn_w2v = 5, decay = 0, nn_pos = 3): 0.8995748423984753\n",
      "Accuracy (nn_w2v = 5, decay = 0, nn_pos = 5): 0.9026535698577921\n",
      "Accuracy (nn_w2v = 5, decay = 1, nn_pos = 0): 0.8840345990323999\n",
      "Accuracy (nn_w2v = 5, decay = 1, nn_pos = 1): 0.8947368421052632\n",
      "Accuracy (nn_w2v = 5, decay = 1, nn_pos = 3): 0.9026535698577921\n",
      "Accuracy (nn_w2v = 5, decay = 1, nn_pos = 5): 0.9042662366221962\n"
     ]
    }
   ],
   "source": [
    "# The cell outputs the accuracy of using our training method on the testing keys (words)\n",
    "# present in the training keys(words). Whenever a testing key is not present in the \n",
    "# training data, it uses the baseline.\n",
    "for nn_w2v in [1, 3, 5]:\n",
    "    for decay in [STRATEGY.AVERAGE, STRATEGY.EXP_DECAY]:\n",
    "        if decay == STRATEGY.AVERAGE:\n",
    "            train_data = load_preprocessed(\"preprocessed_data/train_data_\" + str(nn_w2v) + \".csv\")\n",
    "            test_data = load_preprocessed(\"preprocessed_data/test_data_\" + str(nn_w2v) + \".csv\")\n",
    "        elif nn_w2v > 1:\n",
    "            train_data = load_preprocessed(\"preprocessed_data/train_data_exp_\" + str(nn_w2v) + \".csv\")\n",
    "            test_data = load_preprocessed(\"preprocessed_data/test_data_exp_\" + str(nn_w2v) + \".csv\")\n",
    "        lemma_labels, pos_labels, w2v_labels = return_labels(train_data, test_data)\n",
    "        for nn_pos in [0, 1, 3, 5]:\n",
    "            train_features = get_features(train_data, lemma_labels, pos_labels, w2v_labels, nn_pos)\n",
    "            test_features = get_features(test_data, lemma_labels, pos_labels, w2v_labels, nn_pos)\n",
    "            train_data_dict, train_labels_dict = get_word_based_dicts(train_features, train_data_labels)\n",
    "            test_data_dict, test_labels_dict = get_word_based_dicts(test_features, test_data_labels)\n",
    "\n",
    "            rf_dict = {}\n",
    "            pred_dict = {}\n",
    "            correct = 0\n",
    "            number_of_labels = 0\n",
    "            for key in test_data_dict:\n",
    "                rf = RandomForestClassifier(random_state=42)\n",
    "                try:\n",
    "                    rf.fit(train_data_dict[key], train_labels_dict[key])\n",
    "                    pred_dict[key] = rf.predict(test_data_dict[key])\n",
    "                except:\n",
    "                    pred_dict[key] = baseline_preprocessed(test_data_dict[key], lemma_labels[key], pos_labels)\n",
    "                correct += sum(i==j for i, j in zip(pred_dict[key], test_labels_dict[key]))\n",
    "                number_of_labels += len(pred_dict[key])\n",
    "            \n",
    "            acc_msg = f\"Accuracy (nn_w2v = {nn_w2v}, decay = {decay}, nn_pos = {nn_pos}): {correct / number_of_labels}\"\n",
    "            with open('results.txt', 'a') as f:\n",
    "                f.write(acc_msg + '\\n')\n",
    "            print(acc_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of a: 0.7873015873015873\n",
      "Accuracy of n: 0.8245721271393643\n",
      "Accuracy of o: 0.9947692307692307\n",
      "Accuracy of r: 0.9237668161434978\n",
      "Accuracy of v: 0.8103078024337866\n"
     ]
    }
   ],
   "source": [
    "train_data = load_preprocessed(\"preprocessed_data/train_data_exp_5.csv\")\n",
    "test_data = load_preprocessed(\"preprocessed_data/test_data_exp_5.csv\")\n",
    "lemma_labels, pos_labels, w2v_labels = return_labels(train_data, test_data)\n",
    "train_features = get_features(train_data, lemma_labels, pos_labels, w2v_labels, 5)\n",
    "test_features = get_features(test_data, lemma_labels, pos_labels, w2v_labels, 5)\n",
    "train_data_dict, train_labels_dict = get_word_based_dicts(train_features, train_data_labels)\n",
    "test_data_dict, test_labels_dict = get_word_based_dicts(test_features, test_data_labels)\n",
    "\n",
    "rf_dict = {}\n",
    "pred_dict = {}\n",
    "correct = [0, 0, 0, 0, 0]\n",
    "number_of_labels = [0, 0, 0, 0, 0]\n",
    "for key in test_data_dict:\n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "    try:\n",
    "        rf.fit(train_data_dict[key], train_labels_dict[key])\n",
    "        pred_dict[key] = rf.predict(test_data_dict[key])\n",
    "    except:\n",
    "        pred_dict[key] = baseline_preprocessed(test_data_dict[key], lemma_labels[key], pos_labels)\n",
    "    for idx, w in enumerate(test_data_dict[key]):\n",
    "        if pred_dict[key][idx] == test_labels_dict[key][idx]:\n",
    "            correct[w[0]] += 1\n",
    "        number_of_labels[w[0]] += 1\n",
    "\n",
    "for idx, pos in enumerate(pos_labels):\n",
    "    print(f\"Accuracy of {pos}: {correct[idx]/number_of_labels[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mistakes.txt', 'a') as f:\n",
    "    f.write(\"lemma, predicted, gold standard\\n\")\n",
    "for key in pred_dict:\n",
    "    for idx in range(len(pred_dict[key])):\n",
    "        if pred_dict[key][idx] != test_labels_dict[key][idx]:\n",
    "            with open('mistakes.txt', 'a') as f:\n",
    "                f.write(f\"{lemma_labels[key]}, {pred_dict[key][idx]}, {test_labels_dict[key][idx]}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d8df1c93d52095079b28c314882b9f8223933acddcf9ddd27c0be9d60904f8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
